# -*- coding: utf-8 -*-
"""Avaliacao2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P4-J1UTO2-hIjR1zMnOTnH23PKxv_aNv

# Parte 1
"""

import pandas as pd
import numpy as np

import seaborn as sns

from sklearn import preprocessing

from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, train_test_split
from sklearn.utils.class_weight import compute_class_weight

dropColunas = []
df = pd.read_csv("falencia-treinamento.csv", delimiter=";")

df.drop(["ID"], axis=1, inplace=True)
correlacao = df.corr()
correlacao = np.sqrt(correlacao ** 2)

"""# Parte 2"""

def dropColumns(correlacao, valorCorte, dropColunas):

    # Remove diagonal and lower triangular matrix to avoid duplicate features
    correlacao = correlacao.where(np.triu(np.ones(correlacao.shape), k=1).astype(bool))
    
    # Find features with values higher than cutoff
    high_corr_features = [col for col in correlacao.columns if any(correlacao[col] > valorCorte)]
    dropColunas.extend(high_corr_features)
    # Drop high correlation features
    correlacao = correlacao.drop(high_corr_features, axis=1)
    correlacao = correlacao.drop(high_corr_features, axis=0)
    
    return correlacao

dropColunas = []
print(dropColunas)
dropColumns(correlacao, 0.8, dropColunas)
print(dropColunas)

"""# Parte 3"""

ndf = df.drop(dropColunas, axis=1)
x = ndf.values[:, :-1]
y = ndf.values[:, -1]

cw = compute_class_weight('balanced', classes=[0,1], y=y)

regressao = LogisticRegression(solver="newton-cg", class_weight={0:cw[0], 1:cw[1]})
#regressao.fit(x, y)

arvore = DecisionTreeClassifier(class_weight={0:cw[0], 1:cw[1]})
#arvore.fit(x,y)

composicao = RandomForestClassifier(n_estimators=1000, class_weight={0:cw[0], 1:cw[1]}, max_features="log2")
#composicao.fit(x, y)

"""# Parte 4"""

#cv = KFold(n_splits= 10, shuffle = True)
cv = StratifiedKFold(n_splits=10, shuffle = True)
scoresRegressao = cross_val_score(regressao, x, y, scoring="f1", cv=cv)
print("Regressao:",scoresRegressao)

scoresArvore = cross_val_score(arvore, x, y, scoring="f1", cv=cv)
print("Arvore:",scoresArvore)

scoresComposicao = cross_val_score(composicao, x, y, scoring="f1", cv=cv)
print("Composicao:",scoresComposicao)

def est(score, nomeModelo):
  print("\n\nModelo: " + nomeModelo)
  print("Metodo de Avaliacao: F-Measure")
  print("Media: {:.2f}".format(np.mean(score)))
  print("Desvio Padrao: {:.4f}".format(np.std(score)))

est(scoresRegressao, "Regressao Logistica")
est(scoresArvore, "Arvore de Decisao")
est(scoresComposicao, "Random Forest")

"""# Parte 5"""

def gerarResposta(modelo, dropColunas):
  dropColunas.append("ID")
  df_test = pd.read_csv("falencia-teste.csv", delimiter=";")

  df_test.drop(dropColunas, axis=1, inplace=True)

  x_test = df_test.values[:,:]

  y_test = modelo.predict(x_test)

  y_test = y_test.astype(int)
  resultado = pd.DataFrame(y_test, columns=["Resultado"])
  dropColunas = []

  print(resultado.to_csv(sep=";"))

composicao.fit(x,y)
gerarResposta(composicao, dropColunas)